{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "The `titanic.xls` spreadsheet in the `data` directory contains data regarding the passengers on the Titanic when it sank in 1912. A recent [Kaggle competition](http://www.kaggle.com/c/titanic-gettingStarted) was based on predicting survival for passengers based on the attributes in the passenger list. \n",
    "\n",
    "Use scikit-learn to build both a support vector classifier and a logistic regression model to predict survival on the Titanic. Use cross-validation to assess your models, and try to tune them to improve performance.\n",
    "\n",
    "Discuss the benefits and drawbacks of both approaches for application to such problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_excel(\"/Users/schluetd/Bios8366/data/titanic.xls\", \"titanic\")\n",
    "# Redefine gender as a numeric variable\n",
    "titanic['male'] = titanic.sex.replace({'male':1, 'female':0})\n",
    "titanic_subset = titanic.filter(items = ('survived','pclass','male','age','sibsp','parch','fare')).copy()\n",
    "\n",
    "# Percentages of missing by variable\n",
    "titanic_subset.isnull().sum()/len(titanic_subset)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above, the variables `age` and `fare` have missing values present. (Note that there is only one observation missing `fare`, hence the low percent missingness. We could probably just drop that observation, but since we will be imputing age anyway, we'll also impute fare for that observation.)  \n",
    "\n",
    "We will perform multiple imputation (MI). MI works best when the data are missing at random, or when the missingness mechanism is a function of other data to which we have access. Looking at the correlation matrix below, we can get a sense of whether or not we might be reasonably able to predict age and fare from the other variables in the titanic dataset. We use Spearman rank correlation since class and sex are categorical. Some correlations are particularly large in absolute value, such as between fare and pclass. (We would expect this since a higher fare would be associated with first class compared to third class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use spearman correlation since there are categorical variables\n",
    "titanic_subset.dropna().corr(method='spearman')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform MI, we will use regularized regression to predict age and fare from the remaining variables. Our outcome is included in the model, which is considered best practice to preserve any association between the imputed predictors and the outcome (as noted in *Regression Modeling Strategies*). For this, we will use ElasticNet and vary the ratio of L1 (Lasso) vs L2 (Ridge) regularization. Missing values for a given observation will be replaced with the average of the imputed values for that observation. This takes into account some of the uncertainty involved with imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Only use complete variables to perform imputation - pop off the two variables to be imputed\n",
    "impute_subset = titanic_subset.copy()\n",
    "# Variables we are imputing\n",
    "imp_age = impute_subset.pop('age')\n",
    "imp_fare = impute_subset.pop('fare')\n",
    "\n",
    "# Set up lists to hold imputations\n",
    "age_imp = []\n",
    "fare_imp = []\n",
    "\n",
    "# Determine indexes associated with missing values\n",
    "missing_age = np.isnan(imp_age)\n",
    "missing_fare = np.isnan(imp_fare)\n",
    "\n",
    "# Vary the ratio of L1 (Lasso) and L2 (Ridge) regularization used\n",
    "for l1_l2_ratio in 0.1, 0.25, 0.5, 0.75, 0.9:\n",
    "    \n",
    "    # Fit imputation model for age on the non-missing data\n",
    "    mod_age = ElasticNet(l1_ratio=l1_l2_ratio)\n",
    "    mod_age.fit(impute_subset[~missing_age], imp_age[~missing_age])\n",
    "    # Predict the missing values\n",
    "    imputed_age = mod_age.predict(impute_subset[missing_age])\n",
    "    age_imp.append(imputed_age)\n",
    "\n",
    "    # Fit imputation model for age on the non-missing data\n",
    "    mod_fare = ElasticNet(l1_ratio=l1_l2_ratio)\n",
    "    mod_fare.fit(impute_subset[~missing_fare], imp_fare[~missing_fare])\n",
    "    # Predict the missing values\n",
    "    imputed_fare = mod_fare.predict(impute_subset[missing_fare])\n",
    "    fare_imp.append(imputed_fare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace missing values with mean of the imputed values\n",
    "titanic_subset.loc[missing_age,'age'] = [np.mean(y) for y in zip(*age_imp)]\n",
    "titanic_subset.loc[missing_fare,'fare'] = [np.mean(y) for y in zip(*fare_imp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Want to predict `survived`\n",
    "y = titanic_subset.pop('survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data are complete, we scale the predictors and move on to model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize the predictors\n",
    "from sklearn import preprocessing\n",
    "titanic_scaled = preprocessing.scale(titanic_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier\n",
    "For both the SVC and the logistic regression model, we will use `GridSearchCV` to tune each model for optimal performance. For the SVC, we will tune the `C` parameter, which controls a penalty parameter (lower values indicate a higher degree of regularization), and the kernel function (radial basis function, third degree polynomial, or linear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit SVM with cross validation\n",
    "from sklearn import model_selection, svm\n",
    "# Perform a grid search to tune the model and improve performance\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameters to tune\n",
    "svm_param_grid = {'C': [0.0001,0.001,0.01,0.1,0.5,1,2],\n",
    "                  'kernel': ['rbf','poly','linear']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_mod = svm.SVC()\n",
    "svm_cv = GridSearchCV(svm_mod, svm_param_grid, n_jobs=4, cv = 4).fit(titanic_scaled, y)\n",
    "svm_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above, the optimal parameters appear to be a radial basis kernel function and `C=0.1`. We will now fit an SVC with these parameters. We will use *accuracy* as the metric by which we assess fit. In class we used `f1-weighted`, which is a score method based on precision-recall. This approach works well when there is high class imbalance. Here, however, the imbalance is only around 60% to 40% (died to survived). We will use 4-fold cross validation to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Class imbalance\n",
    "pd.crosstab(y,'survived')/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the SVM based on the identified best parameters above\n",
    "svc = svm.SVC(kernel='rbf', C=0.1)\n",
    "svc_cv_scores = model_selection.cross_val_score(svc, titanic_scaled, y, cv=4,\n",
    "                                            scoring='accuracy')\n",
    "svc_cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"SVC CV Accuracy: %0.2f (+/- %0.2f)\" % (svc_cv_scores.mean(), svc_cv_scores.std()*1.96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "For logistic regression, we will tune the penalty (L1 or L2) and the `C` parameter, which has a function similar to the SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lrm_param_grid = {'penalty': ['l1','l2'],\n",
    "                 'C': [0.0001,0.001,0.01,0.1,0.5,1,2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrm_mod = LogisticRegression()\n",
    "\n",
    "lrm_cv = GridSearchCV(lrm_mod, lrm_param_grid, n_jobs=4, cv=4).fit(titanic_scaled, y)\n",
    "lrm_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters were an L2 (Ridge) penalty with `C=0.001`. We fit a logistic regression model with these parameters and obtain the 4-fold cross validation accuracy estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now fit LRM based on best params\n",
    "lrm = LogisticRegression(penalty='l2', C=0.001)\n",
    "\n",
    "lrm_cv_scores = model_selection.cross_val_score(lrm, titanic_scaled, y, cv=4,\n",
    "                                            scoring='accuracy')\n",
    "lrm_cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Logistic Regression CV Accuracy: %0.2f (+/- %0.2f)\" % (lrm_cv_scores.mean(), lrm_cv_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average CV accuracy was 0.72, slightly lower than that of the SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages/Disadvantages  \n",
    "\n",
    "Logistic regression models (LRMs) have the advantage of being directly interpretable if assessing covariate effects is of interest, while an SVC is more of a \"black box\" method. In this way, LRMs can be preferable since they are simpler models, if you are not sacrificing too much in terms of model performance.\n",
    "\n",
    "The SVM can use a variety of kernel functions to capture nonlinear effects in the data. A LRM, however, assumes a specific functional form. An SVC can also be more robust to outliers. One disadvantage of using a LRM is that multicollinearity of predictors should be taken into account. The models above each used the same predictors for comparison, but we did see that pclass and fare had a high negative association. Below is a plot that describes the relationship visually. SVC is fairly robust to multicollinearity, so predictions from the SVC may be more robust than the LRM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.stripplot(x=\"pclass\", y=\"fare\", data=titanic);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "The file `TNNASHVI.txt` in your data directory contains daily temperature readings for Nashville, courtesy of the [Average Daily Temperature Archive](http://academic.udayton.edu/kissock/http/Weather/). This data, as one would expect, oscillates annually. Using PyMC3, use a Gaussian process to fit a non-parametric regression model to this data, choosing an appropriate covariance function. Plot 10 regression lines drawn from your process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "daily_temps = pd.read_table(\"/Users/schluetd/Bios8366/data/TNNASHVI.txt\", sep='\\s+', \n",
    "                            names=['month','day','year','temp'], na_values=-99)\n",
    "daily_temps.temp.plot(style='b.', figsize=(10,6), grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Drop the rows with na values\n",
    "daily_temps_clean = daily_temps.dropna()\n",
    "\n",
    "temps = daily_temps_clean.temp\n",
    "x, y = temps.reset_index().values.T\n",
    "X = x.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with pm.Model() as sparse_model:\n",
    "    \n",
    "    l1 = pm.HalfCauchy(\"l1\", 5)\n",
    "    η1 = pm.HalfCauchy(\"η1\", 5)\n",
    "    l2 = pm.HalfCauchy(\"l2\", 5)\n",
    "    η2 = pm.HalfCauchy(\"η2\", 5)\n",
    "    \n",
    "    # covariance \n",
    "    cov = (η1**2)*pm.gp.cov.Cosine(1, l1) + (η2**2) * pm.gp.cov.Matern52(1, l2)\n",
    "\n",
    "    gp = pm.gp.MarginalSparse(cov_func=cov, approx=\"FITC\")\n",
    "    \n",
    "    # set inducing points\n",
    "    Xu = pm.gp.util.kmeans_inducing_points(50, X)\n",
    "    \n",
    "    # following 5.1 example\n",
    "    σ = pm.HalfCauchy(\"σ\", beta=2)\n",
    "    obs = gp.marginal_likelihood(\"obs\", X=X, Xu=Xu, y=y, sigma=σ)\n",
    "    \n",
    "    map_est = pm.find_MAP()\n",
    "\n",
    "## For expediency, let's just compute the map\n",
    "X_new = np.linspace(x.min(), x.max(), 600)[:,None]\n",
    "with sparse_model:\n",
    "    f_pred = gp.conditional(\"f_pred\", X_new)\n",
    "    pred_samples = pm.sample_ppc([map_est],\n",
    "                                 vars=[f_pred], \n",
    "                                 samples=10)\n",
    "    \n",
    "\n",
    "fig = plt.figure(figsize=(13,7)); ax = fig.gca()\n",
    "#plot the samples from the gp posterior with samples and shading\n",
    "from pymc3.gp.util import plot_gp_dist\n",
    "plot_gp_dist(ax, pred_samples[\"f_pred\"], X_new)\n",
    "#plot the data and the true latent function\n",
    "plt.plot(X, y, 'ok', ms=3, alpha=0.5, label=\"Observed data\")\n",
    "plt.plot(Xu, 100*np.ones(Xu.shape[0]), \"g.\", ms=5, label=\"Inducing point location\")\n",
    "#axis labels and title\n",
    "plt.xlabel(\"X\")\n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\"); plt.legend();   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Fit a series of random-forest classifiers to the Wisconsin breast cancer dataset (`wisconsin_breast_cancer.csv`), to explore the sensitivity to the parameters `max_features`, the number of variables considered for splitting at each step, `max_depth`, the maximum depth of the tree, and `n_estimators`, the number of trees in the forest. Use appropriate metrics of performance, and include plots against a suitably-chosen range of values for these parameters.\n",
    "\n",
    "Dataset description: Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "- `radius` (mean of distances from center to points on the perimeter) \n",
    "- `texture` (standard deviation of gray-scale values) \n",
    "- `perimeter` \n",
    "- `area` \n",
    "- `smoothness` (local variation in radius lengths) \n",
    "- `compactness` (perimeter^2 / area - 1.0) \n",
    "- `concavity` (severity of concave portions of the contour) \n",
    "- `concave points` (number of concave portions of the contour) \n",
    "- `symmetry` \n",
    "- `fractal dimension` (\"coastline approximation\" - 1)\n",
    "\n",
    "The outcome to be predicted is tumor type (M = malignant, B = benign)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wbc = pd.read_csv(\"/Users/schluetd/Bios8366/data/wisconsin_breast_cancer.csv\")\n",
    "wbc['malignant'] = wbc.diagnosis.replace({'M':1, 'B':0})\n",
    "wisc_bc = wbc.drop('diagnosis', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pop off the outcome\n",
    "X = wisc_bc.copy()\n",
    "y = X.pop('malignant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Class imbalance\n",
    "pd.crosstab(y,'malignant')/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to fit a random forest given parameter values\n",
    "def rf_sens(max_features, n_estimators, max_depth):\n",
    "    rf_temp = RandomForestClassifier(max_features=max_features,\n",
    "                                     n_estimators = n_estimators, \n",
    "                                     max_depth = max_depth,\n",
    "                                     n_jobs=4)\n",
    "    # Perform 4-fold cross validation\n",
    "    cv_res = model_selection.cross_val_score(rf_temp, X, y, \n",
    "                                             cv=4, scoring='accuracy')\n",
    "    \n",
    "    # Return the mean cross-validation accuracy\n",
    "    return cv_res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_features sensitivity\n",
    "max_feat_values = [1, 'sqrt', 10, 20, X.shape[1]]\n",
    "mf_sens_highn_highd = [rf_sens(max_features=max_f, n_estimators=3000, max_depth=4) for max_f in max_feat_values]\n",
    "mf_sens_lown_highd = [rf_sens(max_features=max_f, n_estimators=100, max_depth=4) for max_f in max_feat_values]\n",
    "mf_sens_highn_lowd = [rf_sens(max_features=max_f, n_estimators=3000, max_depth=2) for max_f in max_feat_values]\n",
    "mf_sens_lown_lowd = [rf_sens(max_features=max_f, n_estimators=100, max_depth=2) for max_f in max_feat_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_feat_values[1] = np.sqrt(X.shape[1])\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "l1=plt.plot(max_feat_values, mf_sens_highn_highd, label = 'n_estimators=3000,max_depth=4')\n",
    "l2=plt.plot(max_feat_values, mf_sens_lown_highd, label = 'n_estimators=100,max_depth=4')\n",
    "l3=plt.plot(max_feat_values, mf_sens_highn_lowd, label = 'n_estimators=3000,max_depth=2')\n",
    "l4=plt.plot(max_feat_values, mf_sens_lown_lowd, label = 'n_estimators=100,max_depth=2')\n",
    "\n",
    "plt.xlabel(\"max_features\"); plt.ylabel(\"4-fold CV accuracy\")\n",
    "plt.title(\"max_features sensitivity\"); \n",
    "plt.legend(['n_estimators=3000,max_depth=4',\n",
    "                          'n_estimators=100,max_depth=4',\n",
    "                          'n_estimators=3000,max_depth=2',\n",
    "                          'n_estimators=100,max_depth=2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we see that initially, as the maximum number of features considered grows, the accuracy improves. However, this pattern quickly plateaus and even turns to a decreasing pattern for some of the cases above. In particular, the cases that had a higher max_depth saw a more severe decline. These cases appeared to have higher accuracy for a lower value of max_features, while the cases with a lower max_depth peaked at max_features=20. \n",
    "\n",
    "However, note the y-axis. These trends are all within a two percentage point range. Generally, there doesn't appear to be extreme variation in performance as max_features varies. Empirical evidence has indicated that for classification problems, using max_features=sqrt(n_features) is a good option. It is interesting to note that this value was not optimal for all the situations presented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n_estimators sensitivity\n",
    "n_est_values = [100,500,1000,3000,6000]\n",
    "ne_sens_highmf_highd = [rf_sens(max_features=20, n_estimators=n_est, max_depth=4) for n_est in n_est_values]\n",
    "ne_sens_lowmf_highd = [rf_sens(max_features='sqrt', n_estimators=n_est, max_depth=4) for n_est in n_est_values]\n",
    "ne_sens_highmf_lowd = [rf_sens(max_features=20, n_estimators=n_est, max_depth=2) for n_est in n_est_values]\n",
    "ne_sens_lowmf_lowd = [rf_sens(max_features='sqrt', n_estimators=n_est, max_depth=2) for n_est in n_est_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "l5=plt.plot(n_est_values, ne_sens_highmf_highd, label = 'max_features=20,max_depth=4')\n",
    "l6=plt.plot(n_est_values, ne_sens_lowmf_highd, label = 'max_features=sqrt(n_features),max_depth=4')\n",
    "l7=plt.plot(n_est_values, ne_sens_highmf_lowd, label = 'max_features=20,max_depth=2')\n",
    "l8=plt.plot(n_est_values, ne_sens_lowmf_lowd, label = 'max_features=sqrt(n_features),max_depth=2')\n",
    "plt.xlabel(\"n_estimators\"); plt.ylabel(\"4-fold CV accuracy\")\n",
    "plt.title(\"n_estimators sensitivity\"); \n",
    "plt.legend(['max_features=20,max_depth=4',\n",
    "                          'max_features=sqrt(n_features),max_depth=4',\n",
    "                          'max_features=20,max_depth=2',\n",
    "                          'max_features=sqrt(n_features),max_depth=2']);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows a realtively constant trend of accuracy for each of the cases presented above as the number of estimators changes. The fits that had a larger max_depth tended to peak with a smaller number of estimators. While there was some minor fluctuation in accuracy as a function of n_estimators, the lines are all somewhat flat.\n",
    "\n",
    "Again, note the relatively small difference in the maximum and minimum values of the y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `max_depth`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_depth sensitivity\n",
    "max_depth_values = [1,2,3,4,5]\n",
    "md_sens_highmf_highne = [rf_sens(max_features=20, n_estimators=3000, max_depth=max_dep) for max_dep in max_depth_values]\n",
    "md_sens_lowmf_higne = [rf_sens(max_features='sqrt', n_estimators=3000, max_depth=max_dep) for max_dep in max_depth_values]\n",
    "md_sens_highmf_lone = [rf_sens(max_features=20, n_estimators=100, max_depth=max_dep) for max_dep in max_depth_values]\n",
    "md_sens_lowmf_lone = [rf_sens(max_features='sqrt', n_estimators=100, max_depth=max_dep) for max_dep in max_depth_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "l9=plt.plot(max_depth_values, md_sens_highmf_highne, label = 'max_features=20,n_estimators=3000')\n",
    "l10=plt.plot(max_depth_values, md_sens_lowmf_higne, label = 'max_features=sqrt(n_features),n_estimators=3000')\n",
    "l11=plt.plot(max_depth_values, md_sens_highmf_lone, label = 'max_features=20,n_estimators=100')\n",
    "l12=plt.plot(max_depth_values, md_sens_lowmf_lone, label = 'max_features=sqrt(n_features),n_estimators=100')\n",
    "plt.xlabel(\"max_depth\"); plt.ylabel(\"4-fold CV accuracy\")\n",
    "plt.title(\"max_depth sensitivity\"); \n",
    "plt.legend(['max_features=20,n_estimators=3000',\n",
    "                          'max_features=sqrt(n_features),n_estimators=3000',\n",
    "                          'max_features=20,n_estimators=100',\n",
    "                          'max_features=sqrt(n_features),n_estimators=100']);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, as the max_depth increases, the cross-validated accuracy appears to increase. The lines for all scenarios are largely in agreement, indicating that the number of estimators and max_features don't tend to alter the impact of max_depth. The accuracy increases by about 4% in all cases when comparing the lowest depth (1, i.e. no interactions) with the largest depth presented (5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Use a grid search to optimize the number of estimators and max_depth for a Gradient Boosted Decision tree using the Wisconsin breast cancer data. Plug this optimal ``max_depth`` into a *single* decision tree.  Does this single tree over-fit or under-fit the data? Repeat this for the Random Forest.  Construct a single decision tree using the ``max_depth`` which is optimal for the Random Forest.  Does this single tree over-fit or under-fit the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wbc = pd.read_csv(\"/Users/schluetd/Bios8366/data/wisconsin_breast_cancer.csv\")\n",
    "wbc['malignant'] = wbc.diagnosis.replace({'M':1, 'B':0})\n",
    "wisc_bc = wbc.drop('diagnosis', axis=1)\n",
    "X = wisc_bc.copy()\n",
    "y = X.pop('malignant')\n",
    "\n",
    "# Parameter values to test\n",
    "param_grid = {'n_estimators': [50,100,500,1000,3000,5000],\n",
    "              'max_depth': [1,2,3,4,5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the gradient boosted tree\n",
    "gbt_est = GradientBoostingClassifier()\n",
    "# Use 4-fold cross validation to pick the best parameters\n",
    "gbt_cv = GridSearchCV(gbt_est, param_grid, n_jobs=4, cv=4).fit(X, y)\n",
    "\n",
    "# Best hyperparameter settings\n",
    "gbt_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbt = DecisionTreeClassifier(max_depth = 1)\n",
    "gbt.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbt_pred = gbt.predict(X)\n",
    "\n",
    "pd.DataFrame(confusion_matrix(gbt_pred, y),\n",
    "            index=['pred_benign','pred_malignant'],\n",
    "            columns=['actual_benign','actual_malignant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tree is likely overfit to the dataset because it is just a single instance of a decision tree. It does not incorporate any uncertainty or regularization, as we get from the gradient boosted tree. The training accuracy is relatively high, though it is not a perfect classifier. Perfect accuracy on a training dataset is an almost sure indication of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Fit the random forest classifier\n",
    "rfc_est = RandomForestClassifier()\n",
    "rfc_cv = GridSearchCV(rfc_est, param_grid, n_jobs=4, cv=4).fit(X, y)\n",
    "\n",
    "# best hyperparameter setting\n",
    "rfc_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = DecisionTreeClassifier(max_depth = 5)\n",
    "rfc.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc_pred = rfc.predict(X)\n",
    "\n",
    "pd.DataFrame(confusion_matrix(rfc_pred, y),\n",
    "            index=['pred_benign','pred_malignant'],\n",
    "            columns=['actual_benign','actual_malignant'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tree is definitely overfit to the data. Its training accuracy is far too high, and this same tree would probably not do well generalizing to a new set of observations. Random forests are like averaging over many decision trees. By fitting one alone, we lose any way of accounting for that uncertainty. A single tree is rarely ever a good predictor as it may be difficult to have good accuracy on a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
